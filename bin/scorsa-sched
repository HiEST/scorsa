#!/usr/bin/python
# -*- coding: utf-8 -*-
#
# scorsa-sched -- Simulate the execution of a workload
#
# Given a system configuration and layout, and a workload description,
# simulate the execution of all the jobs in the workload generating a schedule
# that includes when and where the jobs are executed, along with additional
# stats such as fragmentation.
#
# Copyright © 2016 Jordà Polo <jorda.polo@bsc.es>

from __future__ import division

import logging
import argparse
import configparser
import json
import numpy as np

from collections import defaultdict
from itertools import chain

import scorsa
import policies

logging.basicConfig(format="%(message)s", level=logging.ERROR)

ap = argparse.ArgumentParser()
ap.add_argument("-c", "--config", dest="c", required=True,
                help="System configuration file")
ap.add_argument("-l", "--layout", dest="l", required=True,
                help="System layout file")
ap.add_argument("-w", "--workload", dest="w", required=True,
                help="JSON workload file")
args = ap.parse_args()

config = configparser.ConfigParser(delimiters=("="))
config.read(args.c)

length = config.getfloat("simulator", "length")
step = config.getfloat("simulator", "step")
digits = config.getint("simulator", "digits")
families = json.loads(config.get("system", "families"))

layout = scorsa.map_layout(np.genfromtxt(args.l, delimiter=',', dtype=None))
max_dist = scorsa.distance(layout, 0, len(layout) - 1)

with open(args.w) as w:
    jobs = json.load(w)

index = {job["id"]: job for job in jobs}
system = {}

arrivals = defaultdict(list) # job IDs indexed by arrival time
completions = defaultdict(list) # job IDs indexed by completion time

schedule = {} # scheduling decisions indexed by job ID
stats = []

running = [] # current job IDs being executed
pending = [] # current submitted job IDs that haven't been scheduled yet
free = {} # current available resources indexed by family

for f in families:
    r = json.loads(config.get("system.%s" % f, "range"))
    free[f] = defaultdict(list)
    free[f][1] = [[n] for n in range(r[0], r[1])]

for j in jobs:
    s = scorsa.f2step(float(j["arrival"]), step, digits)
    arrivals[s].append(j["id"])

for s in scorsa.rangef(0.0, length, step, digits):
    if s in completions:
        for jid in completions[s]:
            family = schedule[jid]["family"]
            nodes = schedule[jid]["nodes"]
            policies.free_nodes(free, family, nodes)
            running.remove(jid)

    if s in arrivals:
        pending = pending + arrivals[s]

    new = policies.sched_fcfs(config, s, index, pending, free)

    for jid, sched in new.iteritems():
        running.append(jid)
        completions[sched["end"]].append(jid)

    schedule.update(new)

    # measure fragmentation of free slots, and for each running job,
    # normalized by number of CPUs
    cpus = scorsa.list_free_cpus(free)
    f = scorsa.fragmentation(layout, cpus) * len(cpus) / len(layout)
    for jid in running:
        cpus = list(chain.from_iterable(schedule[jid]["nodes"]))
        f += scorsa.fragmentation(layout, cpus) * len(cpus) / len(layout)

    d = 0.0
    for jid in running:
        cpus = list(chain.from_iterable(schedule[jid]["nodes"]))
        job_dist = scorsa.distance(layout, min(cpus), max(cpus))
        min_dist = scorsa.distance(layout, 0, len(cpus) - 1)
        d += job_dist - min_dist
    d = d / len(running) / max_dist if len(running) > 0 else d

    r = 0.0
    for jid in running:
        cpus = list(chain.from_iterable(schedule[jid]["nodes"]))
        r += len(cpus) / len(layout) if schedule[jid]["reused"] else 0.0

    stats.append({"time": s, "frag": f, "dist": d, "reuse": r})

with open("schedule.json", "w") as out:
    json.dump(schedule, out)

with open("stats.json", "w") as out:
    json.dump(stats, out)
